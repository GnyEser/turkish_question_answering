{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ilker/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ilker/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.question_answering import QuestionAnsweringModel\n",
    "from nested_lookup import nested_lookup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "import string\n",
    "import shutil\n",
    "import datetime as dt\n",
    "import torch\n",
    "import random\n",
    "import itertools\n",
    "import sys, os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from glob import glob\n",
    "pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train-v0.1.json' , encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "data_list = list()\n",
    "for i in data['data']:\n",
    "    for j in i['paragraphs']:\n",
    "        for k in j['qas']:\n",
    "            k['is_impossible'] = False\n",
    "            for z in k['answers']:\n",
    "                tmp = z['answer_start']\n",
    "                z['answer_start'] = int(tmp)\n",
    "        data_list.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features:   0%|          | 0/8308 [00:00<?, ?it/s]Could not find answer: '' vs. 'Yahudi'\n",
      "Could not find answer: '' vs. 'Salisik asit, krizarobin ve iyodun, öğrencisi Hulusi Behçet ile birlikte de süblimenin'\n",
      "convert squad examples to features: 100%|██████████| 8308/8308 [00:08<00:00, 1003.87it/s]\n",
      "add example index and unique id: 100%|██████████| 8308/8308 [00:00<00:00, 800530.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb1f74956a540f4b19cbcf3485bfcb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=3.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8caac66ac44e48afae945e7273d24423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=633.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 4.943995Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Running loss: 4.692345Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Running loss: 3.027156Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Running loss: 0.266419\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10457ea745c74e1e86d1e3d9ac2254df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=633.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.236061\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5903af20e54851a0f31b9c32f95499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=633.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.111119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"dbmdz/bert-base-turkish-cased\"\n",
    "model_base = model_name.split(\"/\")[1].split(\"-\")[0]\n",
    "epoch = 3\n",
    "batch_size = 16\n",
    "\n",
    "dirs = glob(\"*\")\n",
    "dirs = pd.Series(dirs)\n",
    "dels = dirs[dirs.isin([\"cache_dir\",\"outputs\",\"runs\"])].to_list()\n",
    "\n",
    "for folder_name in dels:\n",
    "    try:\n",
    "        shutil.rmtree(folder_name)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "model = QuestionAnsweringModel(model_base,\n",
    "                               model_name, \n",
    "                               args = {\"num_train_epochs\" : epoch, \"train_batch_size\" : batch_size},\n",
    "                               use_cuda = True)\n",
    "model.train_model(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soru_sor(soru,id, context,model):\n",
    "    soru = [{\n",
    "        'context': context,\n",
    "\n",
    "        'qas': [\n",
    "            {\n",
    "                'question':soru,\n",
    "                'id':id\n",
    "            }\n",
    "        ]\n",
    "    }]\n",
    "    \n",
    "    return model.predict(soru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tweets(tweet_path, base_tt):\n",
    "\n",
    "    f = open(tweet_path, \"r\", encoding=\"utf-8\")\n",
    "    a = f.read()\n",
    "    a = pd.Series(a.split(\"\\n\"))\n",
    "    tweets = a[a!=\"\"].str.lower().to_list()\n",
    "\n",
    "    base_questions = [\"'a ne oldu\", \"'e ne oldu\"\n",
    "                      \" ne zaman\", \" nerede \", \" neden\", \" ne için\",\n",
    "                      \" kimdir\" , \" kim\" ]\n",
    "    questions = (base_tt + pd.Series(base_questions)).to_list()\n",
    "\n",
    "    question_list = []\n",
    "    context_list = []\n",
    "    id_list = []\n",
    "    for tweet in tweets:\n",
    "        for q in questions:\n",
    "\n",
    "            question_list.append(q)\n",
    "            context_list.append(tweet)\n",
    "\n",
    "    for i in range(1, len(question_list) +1):\n",
    "        id_list.append(str(i).zfill(4))\n",
    "        \n",
    "    return context_list, question_list, id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(tweet_path, base_tt):\n",
    "    context_list, question_list, id_list = prepare_tweets(tweet_path, base_tt)\n",
    "\n",
    "    predicted_answers = []\n",
    "\n",
    "    for i in range(len(context_list)):\n",
    "        answerNprobs = (soru_sor(question_list[i], id_list[i], context_list[i], model))\n",
    "        answer = answerNprobs[0][0][\"answer\"][0]\n",
    "        predicted_answers.append(answer)\n",
    "    pd.DataFrame(predicted_answers).to_csv(base_tt + \"_answers.csv\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "tweet_path = \"tweets/Demetevgar.txt\"\n",
    "base_tt = \"demet evgar\"\n",
    "get_answers(tweet_path, base_tt)\n",
    "\n",
    "tweet_path = \"tweets/serdarhocanınyanındayız.txt\"\n",
    "base_tt = \"serdar hocanın yanındayız\"\n",
    "get_answers(tweet_path, base_tt)\n",
    "\n",
    "tweet_path = \"tweets/YKSyierteleyin.txt\"\n",
    "base_tt = \"YKSyi erteleyin\"\n",
    "get_answers(tweet_path, base_tt)\n",
    "\n",
    "tweet_path = \"tweets/kıdemtazminatımadokunma.txt\"\n",
    "base_tt = \"kıdem tazminatıma dokunma\"\n",
    "get_answers(tweet_path, base_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "\n",
    "context_list, question_list, id_list = prepare_tweets(tweet_path, base_tt)\n",
    "\n",
    "predicted_answers = []\n",
    "\n",
    "for i in range(len(context_list)):\n",
    "    answerNprobs = (soru_sor(question_list[i], id_list[i], context_list[i], model))\n",
    "    answer = answerNprobs[0][0][\"answer\"][0]\n",
    "    predicted_answers.append(answer)\n",
    "pd.DataFrame(predicted_answers).to_csv(base_tt + \"_answers.csv\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(some_list):\n",
    "    for token in some_list:\n",
    "        if token in stopwords.words(\"turkish\"):\n",
    "            del some_list[some_list.index(token)]\n",
    "    return some_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequent_words(tweet_path, base_tt):\n",
    "\n",
    "    df = pd.read_csv(base_tt + \"_answers.csv\")\n",
    "    tt = tweet_path.split(\"/\")[1].split(\".\")[0]\n",
    "\n",
    "    words = []\n",
    "    for t in df[\"0\"].str.split(\" \", expand = True).astype(str).values:\n",
    "        for k in t:\n",
    "            if (k != \"nan\")  & (k != tt) & (k != \"None\") & (k not in tt) & (\"#\" not in k):\n",
    "                words.append(k)\n",
    "\n",
    "    freq = pd.Series(remove_stopwords(words)).value_counts()\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2020 yılında demet evgar'a arı ile ilgili bir şey olmuş, hastanedeymiş ve 40 yaşındaymış\n",
    "* serdarhoca'nın kuzusu varmış :D turizmle ilgili bir şey olmuş ve seviyorlarmış \n",
    "* YKSyierteleyin: sınav olduğu belli öğrencileri ilgilendiriyor ikamet ile ilgili bir problem varmış\n",
    "* işçilerle ilgili emekliliği ilgilendiriyor tamamlayıcı emeklilik umut gelecek keywordler\n",
    "* metrik olarak şu seçilebilir cevaplardaki kelime sayısı/tt'lerdeki kelime sayısı: confidence gibi bir şey olabilir belki tam emin olamadım\n",
    "* soruların ve cevapların sırası belli bu şekilde de anlam çıkarabiliriz most frequent az önce aklıma gelen bir yöntem. mesela hangi sorusuna en çok gelen yanıt şu ne zaman sorusuna bu. bu ve şu ile cümle kurdurabiliriz gibi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2020         15\n",
       "arı          14\n",
       "sana          8\n",
       "hayat         7\n",
       "yaşında       7\n",
       "40            7\n",
       "bana          5\n",
       "yapmaz        5\n",
       "demet         5\n",
       "hastanede     5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_path = \"tweets/Demetevgar.txt\"\n",
    "base_tt = \"demet evgar\"\n",
    "freq = frequent_words(tweet_path, base_tt)\n",
    "print(freq.sum())\n",
    "freq.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "serdarhocayanındayız    47\n",
       "sonuna                   7\n",
       "kadar                    7\n",
       "gelen                    5\n",
       "turizimden               5\n",
       "kuzuların                5\n",
       "sizi                     5\n",
       "düşünüyor,               5\n",
       "seviyor                  5\n",
       "halkı                    5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_path = \"tweets/serdarhocanınyanındayız.txt\"\n",
    "base_tt = \"serdar hocanın yanındayız\"\n",
    "freq = frequent_words(tweet_path, base_tt)\n",
    "print(freq.sum())\n",
    "freq.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sınav         14\n",
       "ikamet        11\n",
       "yakın         11\n",
       "sınavı        11\n",
       "öğrencinin    10\n",
       "bir            8\n",
       "yks            8\n",
       "olsun          8\n",
       "35.000         7\n",
       "empty          7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_path = \"tweets/YKSyierteleyin.txt\"\n",
    "base_tt = \"YKSyi erteleyin\"\n",
    "freq = frequent_words(tweet_path, base_tt)\n",
    "print(freq.sum())\n",
    "freq.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "işçinin          35\n",
       "emeklilik        10\n",
       "bir               8\n",
       "tamamlayıcı       8\n",
       "ben               8\n",
       "işten             7\n",
       "umudu             7\n",
       "geleceği          7\n",
       "alın              7\n",
       "kolaylastirma     7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_path = \"tweets/kıdemtazminatımadokunma.txt\"\n",
    "base_tt = \"kıdem tazminatıma dokunma\"\n",
    "freq = frequent_words(tweet_path, base_tt)\n",
    "print(freq.sum())\n",
    "freq.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
