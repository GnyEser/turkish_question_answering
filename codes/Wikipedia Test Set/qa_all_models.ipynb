{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ilker/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ilker/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.question_answering import QuestionAnsweringModel\n",
    "from nested_lookup import nested_lookup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "import string\n",
    "import shutil\n",
    "import datetime as dt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train-v0.1.json' , encoding='utf-8') as json_file: \n",
    "    data = json.load(json_file)\n",
    "    \n",
    "data_list = list()\n",
    "for i in data['data']:\n",
    "    for j in i['paragraphs']:\n",
    "        for k in j['qas']:\n",
    "            k['is_impossible'] = False\n",
    "            for z in k['answers']:\n",
    "                tmp = z['answer_start']\n",
    "                z['answer_start'] = int(tmp)\n",
    "        data_list.append(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    model_base = model_name.split(\"/\")[1].split(\"-\")[0]\n",
    "    model = QuestionAnsweringModel(model_base, model_name, use_cuda = True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soru_sor(soru,id, context,model):\n",
    "    soru = [{\n",
    "        'context': context,\n",
    "\n",
    "        'qas': [\n",
    "            {\n",
    "                'question':soru,\n",
    "                'id':id \n",
    "            }\n",
    "        ]\n",
    "    }]\n",
    "    \n",
    "    return model.predict(soru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_input(input_file):\n",
    "    # Context into list\n",
    "    f = open(input_file,\"r\",encoding = \"utf-8\")\n",
    "    first_line = f.readline()\n",
    "    del first_line\n",
    "    # context\n",
    "    context_list =    []\n",
    "    context_line = True\n",
    "    for line in f:\n",
    "        if not line.strip():continue\n",
    "        if line.startswith(\"context\"):\n",
    "            context_line = True\n",
    "            a_list = []\n",
    "        elif line.startswith(\"topic\"):\n",
    "            lines = ' '.join(a_list)\n",
    "            context_list.append(lines)\n",
    "            context_line = False\n",
    "        if context_line:\n",
    "            a_list.append(line.rstrip(\"\\n\").lstrip(\"\\ufeff\").strip(\"|\").lstrip(\"context:\").lstrip(\": \").lstrip(\" \"))\n",
    "    # questions into list\n",
    "    f = open(input_file,\"r\",encoding = \"utf-8\")\n",
    "    question_list =    []\n",
    "    question_line = True\n",
    "    for line in f:\n",
    "        if not line.strip():continue\n",
    "        if line.startswith(\"question\"):\n",
    "            question_line = True\n",
    "            a = []\n",
    "        elif line.startswith(\"answer\"):\n",
    "            lines = ' '.join(a)    \n",
    "            question_list.append(lines)\n",
    "            question_line = False\n",
    "        else:continue\n",
    "        if question_line:\n",
    "            a.append(line.rstrip(\"\\n\").lstrip(\"\\ufeff\").strip(\"|\").lstrip(\"question:\").lstrip(\" \").rstrip(\" \"))\n",
    "    # id into list\n",
    "    f = open(input_file,\"r\",encoding = \"utf-8\")\n",
    "    id_list =    []\n",
    "    id_line = True\n",
    "    for line in f:\n",
    "        if not line.strip():continue\n",
    "        if line.startswith(\"id\"):\n",
    "            id_line = True\n",
    "        else:continue\n",
    "        if id_line:\n",
    "            id_list.append(line.rstrip(\"\\n\").lstrip(\"id:\").strip(\"|\").strip(\" \"))\n",
    "\n",
    "    # To find number of questions for each topic\n",
    "    f = open(input_file,\"r\",encoding = \"utf-8\")\n",
    "    topic = []\n",
    "    topic_line = True\n",
    "    for line in f:\n",
    "        if not line.strip():continue\n",
    "        if line.startswith(\"topic\"):\n",
    "            topic_line = True\n",
    "            a = []\n",
    "        elif line.startswith(\"question\"):\n",
    "            lines = ' '.join(a)\n",
    "            topic.append(lines)\n",
    "            topic_line = False\n",
    "        else:continue\n",
    "        if topic_line:\n",
    "            a.append(line.rstrip(\"\\n\").lstrip(\"topic:\").rstrip(\"|\").rstrip(\" \").strip(\" \"))\n",
    "    # answers\n",
    "    f = open(input_file,\"r\",encoding = \"utf-8\")\n",
    "    answer_list =    []\n",
    "    answer_line = True\n",
    "    for line in f:\n",
    "        if not line.strip():continue\n",
    "        if line.startswith(\"answer\"):\n",
    "            answer_line = True\n",
    "            a = []\n",
    "        elif line.startswith(\"id\"):\n",
    "            lines = ' '.join(a)    \n",
    "            answer_list.append(lines)\n",
    "            answer_line = False\n",
    "        else:continue\n",
    "        if answer_line:\n",
    "            a.append(line.rstrip(\"\\n\").strip(\"|\").lstrip(\"answer:\").lstrip(\" \").rstrip(\" \"))    \n",
    "    # make contexts as much as multiplied by their questions\n",
    "    topic_counter_dict = {}\n",
    "    topic_counts = []\n",
    "    for top in topic:\n",
    "        if not top in topic_counter_dict.keys():\n",
    "            topic_counter_dict[top] = 1\n",
    "        elif top in topic_counter_dict.keys():\n",
    "            topic_counter_dict[top] += 1\n",
    "    for key,value in topic_counter_dict.items():\n",
    "        topic_counts.append(value)\n",
    "    a = [[con]*topic_counts[i] for i,con in enumerate(context_list)]\n",
    "    context_list_with_num = []\n",
    "    for context_list in a:\n",
    "        for context_repeat in context_list:\n",
    "            context_list_with_num.append(context_repeat)\n",
    "    return context_list_with_num, question_list, id_list, topic, answer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(some_list):\n",
    "    for token in some_list:\n",
    "        if token in stopwords.words(\"turkish\"):\n",
    "            del some_list[some_list.index(token)]\n",
    "    return some_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match(predicted, actual):\n",
    "    matched_answers = []\n",
    "    guessed_true = 0\n",
    "    for i, answer in enumerate(actual):\n",
    "        if predicted[i].lower().translate(str.maketrans('', '', string.punctuation)).strip(\" \") == actual[i].lower().translate(str.maketrans('', '', string.punctuation)).strip(\" \"):\n",
    "            guessed_true += 1 \n",
    "            matched_answers.append(predicted[i])\n",
    "    return guessed_true,matched_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train-v0.1.json' , encoding='utf-8') as json_file: \n",
    "    data = json.load(json_file)\n",
    "    \n",
    "data_list = list()\n",
    "for i in data['data']:\n",
    "    for j in i['paragraphs']:\n",
    "        for k in j['qas']:\n",
    "            k['is_impossible'] = False\n",
    "            for z in k['answers']:\n",
    "                tmp = z['answer_start']\n",
    "                z['answer_start'] = int(tmp)\n",
    "        data_list.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2232"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qas': [{'question': 'el-Hasan b. Muhammed el-Vezzan isimli bilgin avrupa’da nasıl tanınmaktadır ?',\n",
       "   'id': 1,\n",
       "   'answers': [{'answer_start': 171, 'text': 'Afrikalı Leo'}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'el-Hasan b. Muhammed el-Vezzan hangi şehirde büyümüştür ?',\n",
       "   'id': 2,\n",
       "   'answers': [{'answer_start': 278, 'text': 'Fas (Fez) şehrinde'}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'el-Hasan b. Muhammed el-Vezzan sicilyalı korsanların eline düştüğünde nerelere satılmıştır ?',\n",
       "   'id': 3,\n",
       "   'answers': [{'answer_start': 587,\n",
       "     'text': 'ilk olarak Napoli’ye daha sonra Roma’ya'}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'el-Hasan b. Muhammed el-Vezzan kim tarafından vaftiz edilmiştir ?',\n",
       "   'id': 4,\n",
       "   'answers': [{'answer_start': 635, 'text': 'Papa X. Leo'}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'el-Hasan b. Muhammed el-Vezzan kaç yılında vaftiz edilmiştir ?',\n",
       "   'id': 5,\n",
       "   'answers': [{'answer_start': 658, 'text': '6.1.1520 yılında'}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'el-Hasan b. Muhammed el-Vezzan hangi isimle vaftiz edilmiştir ?',\n",
       "   'id': 6,\n",
       "   'answers': [{'answer_start': 698, 'text': 'Giovanni Leo olarak'}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'el-Hasan b. Muhammed el-Vezzan Afrika kitabını hangi yılda tamamlamıştır ?',\n",
       "   'id': 7,\n",
       "   'answers': [{'answer_start': 1005, 'text': '1526’da'}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'el-Hasan b. Muhammed el-Vezzan Tunus’a ne zaman dönmüştür ?',\n",
       "   'id': 8,\n",
       "   'answers': [{'answer_start': 1040, 'text': '935/1529 yılında'}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'el-Hasan b. Muhammed el-Vezzan yazar olarak faaliyetlerini nerelerde devam ettirmiştir?',\n",
       "   'id': 9,\n",
       "   'answers': [{'answer_start': 831, 'text': 'Roma ve Bologna’da'}],\n",
       "   'is_impossible': False}],\n",
       " 'context': 'İslam dünyasında bilimin 16. yüzyılda hala yüksek seviyede bulunduğunu gösteren çok ilginç bir örneği deskriptif coğrafya ekolünden verebiliriz. Bize bu örneği, Avrupa’da Afrikalı Leo (Leo Africanus) olarak tanınan el-hasan b. Muhammed el-Vezzan (doğumu yaklaşık 888/1483)’dır. Fas (Fez) şehrinde büyümüş ve eğitimini almış olan Granada doğumlu bu bilgin, diplomatik hizmetler yoluyla, özellikle kuzey Afrika’da olmak üzere birçok İslam ülkesini tanıyıp bir yazar olarak coğrafya ve kartografya ile ilgileniyordu. İstanbul’dan dönüş yolculuğunda Sicilyalı korsanların eline esir düşmüş, ilk olarak Napoli’ye daha sonra Roma’ya satılıp Papa X. Leo tarafından 6.1.1520 yılında bizzat Papa’nın adıyla Giovanni Leo olarak vaftiz edilmişti. İtalya’daki ikameti sırasında İtalyanca öğrendi ve Arapça öğretti. Yazar olarak faaliyetlerini Roma ve Bologna’da devam ettirdi. Afrika coğrafyası dışında kuzey Afrikalı 30 bilginin biyografilerini içeren diğer bir eser derledi. Afrika kitabını esaretinin 6. yılı olan 1526’da İtalyan dilinde tamamladı. 935/1529 yılında Tunus’a döndü ve orada Müslüman olarak öldü.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_list_with_num, question_list, id_list, topic, answer_list = test_data_input(\"test_questions_compiled.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pretrained_models = [\"dbmdz/bert-base-turkish-128k-cased\",\n",
    "\"dbmdz/bert-base-turkish-128k-uncased\",\n",
    "\"dbmdz/bert-base-turkish-cased\",\n",
    "\"dbmdz/bert-base-turkish-uncased\",\n",
    "\"dbmdz/distilbert-base-turkish-cased\",\n",
    "\"dbmdz/electra-base-turkish-cased-discriminator\",\n",
    "\"dbmdz/electra-small-turkish-cased-discriminator\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%capture\n",
    "model = get_model(pretrained_models[0])\n",
    "model.train_model(data_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "context_list_with_num, question_list, id_list, topic, answer_list = test_data_input(\"test_questions_compiled.txt\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%capture\n",
    "predicted_answers = []\n",
    "\n",
    "for i in range(len(context_list_with_num)):\n",
    "    answerNprobs = (soru_sor(question_list[i], id_list[i], context_list_with_num[i], model))\n",
    "    answer = answerNprobs[0][0][\"answer\"][0]\n",
    "    predicted_answers.append(answer)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tokenized_preds = [word_tokenize(sentence) for sentence in predicted_answers]\n",
    "tokenized_answers = [word_tokenize(sentence) for sentence in answer_list]\n",
    "predictions = [\" \".join(remove_stopwords(token_list)) for token_list in tokenized_preds]\n",
    "actual_answers = [\" \".join(remove_stopwords(token_list)) for token_list in tokenized_answers]\n",
    "exact_match_number, exact_match_answers = exact_match(predictions, actual_answers)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"Exact_match_number:{}\".format(exact_match_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function for the whole process\n",
    "def find_exact_match_number(model_name):\n",
    "    \n",
    "    t1 = dt.datetime.now()\n",
    "    \n",
    "    \"\"\"\n",
    "    istediginiz bir şey varsa (predictions, tokenized_preds gibi) return ettirebilirsiniz\n",
    "    en başta cache_dir, outputs, runs klasörlerini sildiriyorum. çünkü bir model oluşturunca\n",
    "    bunları da oluşturuyor ve bu klasörleri silmeden yeni model çağıramıyor\n",
    "    %% capture her şeyi bastırmaması için (verbose = 0 gibi)\n",
    "    \"\"\"\n",
    "    dirs = !ls\n",
    "    dirs = pd.Series(dirs)\n",
    "    dels = dirs[dirs.isin([\"cache_dir\",\"outputs\",\"runs\"])].to_list()\n",
    "\n",
    "    for folder_name in dels:\n",
    "        try:\n",
    "            shutil.rmtree(folder_name)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    model = get_model(pretrained_models[0])\n",
    "    model.train_model(data_list)\n",
    "    context_list_with_num, question_list, id_list, topic, answer_list = test_data_input(\"test_questions_compiled.txt\")\n",
    "    \n",
    "    predicted_answers = []\n",
    "\n",
    "    for i in range(len(context_list_with_num)):\n",
    "        answerNprobs = (soru_sor(question_list[i], id_list[i], context_list_with_num[i], model))\n",
    "        answer = answerNprobs[0][0][\"answer\"][0]\n",
    "        predicted_answers.append(answer)\n",
    "    \n",
    "    tokenized_preds = [word_tokenize(sentence) for sentence in predicted_answers]\n",
    "    tokenized_answers = [word_tokenize(sentence) for sentence in answer_list]\n",
    "    predictions = [\" \".join(remove_stopwords(token_list)) for token_list in tokenized_preds]\n",
    "    actual_answers = [\" \".join(remove_stopwords(token_list)) for token_list in tokenized_answers]\n",
    "    exact_match_number, exact_match_answers = exact_match(predictions, actual_answers)\n",
    "    \n",
    "    t2 = dt.datetime.now()\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return exact_match_number, t1, t2, predicted_answers, answer_list\n",
    "\n",
    "def print_results(model_name, exact_match_number, t1, t2):\n",
    "    \n",
    "    print(model_name)\n",
    "    print(\"Exact_match_number:{}\".format(exact_match_number))\n",
    "    print(\"Training + prediction took: \" + str(t2-t1).split(\".\")[0] + \" (format: hh:mm:ss)\")\n",
    "    print(\" \") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_models = [\"dbmdz/bert-base-turkish-128k-cased\",\n",
    "\"dbmdz/bert-base-turkish-128k-uncased\",\n",
    "\"dbmdz/bert-base-turkish-cased\",\n",
    "\"dbmdz/bert-base-turkish-uncased\",\n",
    "\"dbmdz/distilbert-base-turkish-cased\",\n",
    "\"dbmdz/electra-base-turkish-cased-discriminator\",\n",
    "\"dbmdz/electra-small-turkish-cased-discriminator\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "model_name = pretrained_models[0]\n",
    "exact_match_number, t1, t2, predicted_answers, answer_list = find_exact_match_number(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res = pd.DataFrame(predicted_answers + [str(t2-t1).split(\".\")[0]] , columns = [\"predicted\"])\n",
    "res[\"true\"] = answer_list + [model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Gazeteci-Yazar Kaya Muzaffer Ilıcak</td>\n",
       "      <td>Gazeteci-Yazar Kaya Muzaffer Ilıcak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Gazeteci-Yazar Kaya Muzaffer Ilıcak</td>\n",
       "      <td>Cihat Uskan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td></td>\n",
       "      <td>Shotokan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td></td>\n",
       "      <td>Türkiye Üniversite Sporları Federasyonu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0:03:48</td>\n",
       "      <td>dbmdz/bert-base-turkish-128k-cased</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               predicted  \\\n",
       "105  Gazeteci-Yazar Kaya Muzaffer Ilıcak   \n",
       "106  Gazeteci-Yazar Kaya Muzaffer Ilıcak   \n",
       "107                                        \n",
       "108                                        \n",
       "109                              0:03:48   \n",
       "\n",
       "                                        true  \n",
       "105      Gazeteci-Yazar Kaya Muzaffer Ilıcak  \n",
       "106                              Cihat Uskan  \n",
       "107                                 Shotokan  \n",
       "108  Türkiye Üniversite Sporları Federasyonu  \n",
       "109       dbmdz/bert-base-turkish-128k-cased  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results/dbmdz_bert-base-turkish-128k-cased_default'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"results/\" + (model_name + \"_default\").replace(\"/\", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv(\"results/\" + (model_name + \"_default.csv\").replace(\"/\", \"_\"), index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_models[0] in pretrained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
